name: "aws-datadog-logs-archive"
# Canonical GitHub repo
github_repo: "cloudposse-terraform-components/aws-datadog-logs-archive"
# Short description of this project
description: |-
  This component is responsible for provisioning Datadog Log Archives. It creates a single log archive pipeline for each
  AWS account. If the `catchall` flag is set, it creates a catchall archive within the same S3 bucket.

  Each log archive filters for the tag `env:$env` where $env is the environment/account name (ie sbx, prd, tools, etc), as
  well as any tags identified in the additional_tags key. The `catchall` archive, as the name implies, filters for '\*'.

  A second bucket is created for cloudtrail, and a cloudtrail is configured to monitor the log archive bucket and log
  activity to the cloudtrail bucket. To forward these cloudtrail logs to datadog, the cloudtrail bucket's id must be added
  to the s3_buckets key for our datadog-lambda-forwarder component.

  Both buckets support object lock, with overridable defaults of COMPLIANCE mode with a duration of 7 days.

  ## Prerequisites

  - Datadog integration set up in target environment
    - We rely on the datadog api and app keys added by our datadog integration component

  ## Issues, Gotchas, Good-to-Knows

  ### Destroy/reprovision process

  Because of the protections for S3 buckets, if we want to destroy/replace our bucket, we need to do so in two passes or
  destroy the bucket manually and then use terraform to clean up the rest. If reprovisioning a recently provisioned
  bucket, the two-pass process works well. If the bucket has a full day or more of logs, though, deleting it manually
  first will avoid terraform timeouts, and then the terraform process can be used to clean up everything else.

  #### Two step process to destroy via terraform

  - first set `s3_force_destroy` var to true and apply
  - next set `enabled` to false and apply or use tf destroy

usage: |-
  **Stack Level**: Global

  Here's an example snippet for how to use this component. It's suggested to apply this component to all accounts from
  which Datadog receives logs.

  ```yaml
  components:
    terraform:
      datadog-logs-archive:
        settings:
          spacelift:
            workspace_enabled: true
        vars:
          enabled: true
    #       additional_query_tags:
    #         - "forwardername:*-dev-datadog-lambda-forwarder-logs"
    #         - "account:123456789012"
  ```

references:
  - name: cloudposse/s3-bucket/aws
    description: Cloud Posse's S3 component
    url: https://registry.terraform.io/modules/cloudposse/s3-bucket/aws/latest
  - name: datadog_logs_archive resource
    description: Datadog's provider documentation for the datadog_logs_archive resource
    url: https://registry.terraform.io/providers/DataDog/datadog/latest/docs/resources/logs_archive
tags:
  - component/datadog-logs-archive
  - layer/datadog
  - provider/aws
  - provider/datadog
# Categories of this project
categories:
  - component/datadog-logs-archive
  - layer/datadog
  - provider/aws
  - provider/datadog
# License of this project
license: "APACHE2"
# Badges to display
badges:
  - name: Latest Release
    image: https://img.shields.io/github/release/cloudposse-terraform-components/aws-datadog-logs-archive.svg?style=for-the-badge
    url: https://github.com/cloudposse-terraform-components/aws-datadog-logs-archive/releases/latest
  - name: Slack Community
    image: https://slack.cloudposse.com/for-the-badge.svg
    url: https://slack.cloudposse.com
related:
  - name: "Cloud Posse Terraform Modules"
    description: Our collection of reusable Terraform modules used by our reference architectures.
    url: "https://docs.cloudposse.com/modules/"
  - name: "Atmos"
    description: "Atmos is like docker-compose but for your infrastructure"
    url: "https://atmos.tools"
contributors: [] # If included generates contribs
